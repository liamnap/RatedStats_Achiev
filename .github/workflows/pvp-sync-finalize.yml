name: Finalize PvP Sync
run-name: Finalize PvP Sync for ${{ inputs.region }}

# Trigger whenever the "PvP Sync Batch" workflow finishes on dev
on:
  workflow_run:
    workflows: ["PvP Sync Batch"]
    types: [completed]
  workflow_dispatch:
    inputs:
      region:
        description: "Region to finalize"
        required: true
        type: choice
        options: [kr, tw, eu, us]
      sha:
        description: "Head SHA of the batch runs to finalize"
        required: true
        type: string
      branch:
        description: "Branch the batch runs executed on"
        required: true
        type: string
      group_id:
        description: "Dispatcher group id to correlate this cycle"
        required: false
        type: string
      total_batches:
        description: "How many batches to expect (optional; autodetected if omitted)"
        required: false
        type: string

permissions:
  contents: write
  actions: read

jobs:
  guard:
    # fire on: (a) workflow_dispatch, or (b) any "PvP Sync Batch" completion (all regions)
    if: >
      (github.event_name == 'workflow_dispatch') ||
      (github.event.workflow_run.name == 'PvP Sync Batch')
    name: Check if this was the last batch
    runs-on: ubuntu-latest
    concurrency:
       # one finalize per region+sha regardless of trigger type
       # 'needs' isn't available inside the guard job; use a sha-only key here
       group: guard-${{ github.event.workflow_run.head_sha || inputs.sha || github.sha }}
       cancel-in-progress: false
    outputs:
      is_final: ${{ steps.parse.outputs.is_final }}
      region:   ${{ steps.parse.outputs.region }}
      total:    ${{ steps.parse.outputs.total }}
      group_id: ${{ steps.parse.outputs.group_id }}
    steps:
      - name: Determine region + whether this is the final batch
        id: parse
        shell: bash
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          set -euo pipefail
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "region=${{ inputs.region }}" >>"$GITHUB_OUTPUT"
            echo "is_final=true" >>"$GITHUB_OUTPUT"
            # propagate total if provided on dispatch
            if [[ -n "${{ inputs.total_batches }}" ]]; then
              echo "total=${{ inputs.total_batches }}" >>"$GITHUB_OUTPUT"
            fi
            # pass-through group_id if provided
            if [[ -n "${{ inputs.group_id }}" ]]; then
              echo "group_id=${{ inputs.group_id }}" >>"$GITHUB_OUTPUT"
            fi
            echo "üü¢ Finalize via dispatch: region='${{ inputs.region }}'"
            exit 0
          fi

          TITLE="${{ github.event.workflow_run.display_title }}"
          echo "Run title: $TITLE"
          # Expect: "PvP Sync Batch 3 / 10 for kr"
          if [[ "$TITLE" =~ ^PvP[[:space:]]Sync[[:space:]]Batch[[:space:]]([0-9]+)[[:space:]]*/[[:space:]]*([0-9]+)[[:space:]]for[[:space:]]([a-z]{2})$ ]]; then
            CUR="${BASH_REMATCH[1]}"; TOT="${BASH_REMATCH[2]}"; REGION="${BASH_REMATCH[3]}"
            echo "region=$REGION" >>"$GITHUB_OUTPUT"
            echo "total=$TOT"     >>"$GITHUB_OUTPUT"
            if [[ "$CUR" == "$TOT" ]]; then
              echo "is_final=true"  >>"$GITHUB_OUTPUT"
              echo "‚úÖ Detected final batch ($CUR/$TOT) for $REGION"
              # Try to discover group id from THIS completed run's artifacts
              rid="${{ github.event.workflow_run.id }}"
              repo="${GITHUB_REPOSITORY:-${{ github.repository }}}"
              gid="$(gh api repos/"$repo"/actions/runs/"$rid"/artifacts \
                       --jq '.artifacts[].name
                         | capture("pvp-db-[a-z]+-batch-[0-9]+-group-(?<gid>.+)$")?.gid' \
                       | sort -u | head -n1 || true)"
              if [[ -n "$gid" ]]; then
                echo "group_id=$gid" >>"$GITHUB_OUTPUT"
                echo "üîé Detected group_id: $gid"
              fi
            else
              echo "is_final=false" >>"$GITHUB_OUTPUT"
              echo "‚è≠ Not final batch ($CUR/$TOT) for $REGION ‚Üí skipping finalize."
            fi
          else
            echo "is_final=false" >>"$GITHUB_OUTPUT"
            echo "‚ö†Ô∏è Couldn't parse batch info; aborting finalize."
          fi

  finalize:
    if: >
      needs.guard.outputs.is_final == 'true' &&
      (
        (github.event_name == 'workflow_run'     && github.event.workflow_run.conclusion == 'success')
        ||
        (github.event_name == 'workflow_dispatch')
      )
    needs: guard
    concurrency:
      # Here we *can* use needs.guard.outputs.region because this job depends on 'guard'
      group: finalize-${{ needs.guard.outputs.region }}-${{ github.event.workflow_run.head_sha || inputs.sha || github.sha }}
      cancel-in-progress: false
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 1
          ref: ${{ github.event.workflow_run.head_branch || inputs.branch }}
          lfs: false   # don't fail if LFS objects were cleaned up

      - name: (Optional) Hydrate LFS if available, but never fail
        env:
          # Don‚Äôt auto-download LFS during checkout (keeps pointers only)
          GIT_LFS_SKIP_SMUDGE: "1"
        run: |
          set +e
          git lfs install --local
          git lfs fetch
          git lfs checkout
          # If LFS server has pruned objects you‚Äôll see 410/404; we ignore it.
          echo "LFS hydration attempted (continuing even if it failed)."

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'

      - name: Enable Git LFS
        run: |
          git lfs install --local
          git lfs env

      - name: Install runtime deps
        run: |
          python -m pip install --upgrade pip
          pip install aiohttp requests psutil

      - name: Check disk usage
        run: df -h

      - name: Prepare large workspace (/mnt with sudo or RUNNER_TEMP)
        shell: bash
        run: |
          set -euo pipefail
          ROOT="/mnt/gha"
          if command -v sudo >/dev/null 2>&1 && sudo -n true 2>/dev/null; then
            # create /mnt/gha and give current user write perms
            sudo install -d -o "$USER" -g "$USER" -m 0775 "$ROOT"
            echo "Using $ROOT (separate volume)"
          else
            ROOT="${RUNNER_TEMP}/gha"
            mkdir -p "$ROOT"
            echo "Using $ROOT (RUNNER_TEMP fallback)"
          fi
          mkdir -p "$ROOT/_dl" "$ROOT/partial_outputs"
          {
            echo "ROOT=$ROOT"
            echo "DL=$ROOT/_dl"
            echo "OUTDIR=$ROOT/partial_outputs"
          } >> "$GITHUB_ENV"
          df -h

      - name: Wait for ALL region batch runs to complete
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          wf="PvP Sync Batch"
          region="${{ needs.guard.outputs.region }}"
          sha="${{ github.event.workflow_run.head_sha }}"
          branch="${{ github.event.workflow_run.head_branch }}"
          # Fallbacks for workflow_dispatch
          if [[ -z "$sha" ]]; then sha="${{ inputs.sha }}"; fi
          if [[ -z "$branch" ]]; then branch="${{ inputs.branch }}"; fi
          echo "Waiting for all '$wf' runs for region=$region, sha=$sha..."
          while true; do
            # find any runs still pending/running for this sha+region
            rem=$(gh run list \
                  -w "$wf" -b "$branch" -L 500 \
                  --json status,displayTitle,headSha \
                  --jq "[ .[]
                          | select(.headSha==\"$sha\")
                          | select(.displayTitle|endswith(\" for $region\"))
                          | select(.status!=\"completed\") ] | length")
            if (( rem == 0 )); then
              echo "‚úÖ All $wf runs for $region/$sha are done."
              break
            fi
            echo "‚è≥ $rem runs still in progress‚Ä¶"
            sleep 10
          done

      - name: Download every batch‚Äôs DB shard (no globs; use artifact names)
        shell: bash
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          set -euo pipefail

          wf="PvP Sync Batch"
          region="${{ needs.guard.outputs.region }}"
          sha="${{ github.event.workflow_run.head_sha || inputs.sha }}"
          branch="${{ github.event.workflow_run.head_branch || inputs.branch }}"
          expected="${{ needs.guard.outputs.total }}"
          repo="${GITHUB_REPOSITORY:-${{ github.repository }}}"

          root="${ROOT:-${RUNNER_TEMP}/gha}"
          dl="${DL:-$root/_dl}"
          outdir="${OUTDIR:-$root/partial_outputs}"
          rm -rf "$dl" "$outdir"
          mkdir -p "$dl" "$outdir"

          disk_report() {
            echo "------ DISK REPORT (after batch $1) ------"
            df -h
            echo "-- Top dirs in $root --"
            du -x -h -d1 "$root" 2>/dev/null | sort -hr | head -n 10 || true
            echo "-- Top dirs in workspace --"
            du -x -h -d1 "$GITHUB_WORKSPACE" 2>/dev/null | sort -hr | head -n 10 || true
            echo "-- Largest files (workspace + $root) --"
            { find "$GITHUB_WORKSPACE" -type f -size +1M -printf '%s\t%p\n' 2>/dev/null
              find "$root"            -type f -size +1M -printf '%s\t%p\n' 2>/dev/null; } \
              | sort -nr 2>/dev/null | head -n 10 | awk '{printf "%7.1f MB\t%s\n", $1/1048576, $2}' || true
            echo "------------------------------------------"
          }

          # Completed run IDs for this commit/branch (newest first)
          # If we have a group_id, don't restrict by commit (batches may span SHAs).
          # Otherwise, fall back to the last-batch commit like before.
          group_id="${{ needs.guard.outputs.group_id }}"
          if [[ -n "$group_id" ]]; then
            mapfile -t ids < <(gh run list \
              --workflow "$wf" --branch "$branch" \
              --status completed -L 500 --json databaseId \
              --jq '.[].databaseId')
          else
            mapfile -t ids < <(gh run list \
              --workflow "$wf" --branch "$branch" --commit "$sha" \
              --status completed -L 500 --json databaseId \
              --jq '.[].databaseId')
          fi

          # Build: batch_index -> artifact_name and run_id
          declare -A art_by_batch
          declare -A run_by_batch

          for id in "${ids[@]}"; do
            # List artifact names for the run (REST: list workflow run artifacts)
            mapfile -t names < <(gh api repos/"$repo"/actions/runs/"$id"/artifacts \
              --paginate --jq '.artifacts[].name')
          
            for nm in "${names[@]}"; do
              if [[ -n "$group_id" ]]; then
                if [[ "$nm" =~ ^pvp-db-${region}-batch-([0-9]+)-group-${group_id}$ ]]; then
                  idx="${BASH_REMATCH[1]}"
                else
                  continue
                fi
              else
                if [[ "$nm" =~ ^pvp-db-${region}-batch-([0-9]+) ]]; then
                  idx="${BASH_REMATCH[1]}"
                else
                  continue
                fi
              fi
          
              if [[ -z "${art_by_batch[$idx]+x}" ]]; then
                art_by_batch[$idx]="$nm"
                run_by_batch[$idx]="$id"
              fi
            done
          done

          # Determine expected count if not provided
          min_idx=999999; max_idx=-1
          for k in "${!art_by_batch[@]}"; do
            (( k < min_idx )) && min_idx="$k"
            (( k > max_idx )) && max_idx="$k"
          done
          # If caller provided total_batches, trust that; else derive.
          if [[ -z "${expected:-}" || "${expected}" == "0" ]]; then
            if (( max_idx >= 0 )); then expected=$(( max_idx - min_idx + 1 )); else expected=0; fi
          fi
          if (( expected == 0 )); then
            echo "‚ùå No matching artifacts found for region=$region / sha=$sha"
            exit 1
          fi
          echo "Expecting ${expected} shard(s) for region=${region} (index base = ${min_idx})"

          # Download by exact artifact name (no globs)
          for n in $(seq "$min_idx" "$max_idx"); do
            nm="${art_by_batch[$n]:-}"
            id="${run_by_batch[$n]:-}"
            if [[ -z "$nm" || -z "$id" ]]; then
              echo "‚ùå Missing artifact for batch index $n"
              exit 1
            fi
            echo "== downloading index $n : '$nm' from run $id =="
            rm -rf "$dl" && mkdir -p "$dl"
            gh run download "$id" --name "$nm" -D "$dl"
            shopt -s globstar nullglob
            moved=0
            for f in "$dl"/**/achdb_${region}_b*.sqlite; do
              mv -f "$f" "$outdir/"
              moved=1
            done
            shopt -u globstar
            if [[ "$moved" != "1" ]]; then
              echo "‚ùå Did not find a single sqlite shard in artifact '$nm' (index $n)"
              find "$dl" -maxdepth 3 -type f -print || true
              exit 1
            fi
            rm -rf "$dl"
            disk_report "$n"
          done

          ln -sfn "$outdir" partial_outputs
          echo "Found shards:"
          ls -1 partial_outputs

          # Strict check: require a contiguous 0..(expected-1) set
          declare -A have=()
          while IFS= read -r bn; do
            have["$bn"]=1
          done < <(ls -1 partial_outputs/achdb_${region}_b*.sqlite 2>/dev/null \
                   | sed -n 's/.*_b\([0-9]\+\)\.sqlite/\1/p' | sort -n | uniq)
          missing=()
          for i in $(seq 0 $((expected-1))); do
            [[ -n "${have[$i]+x}" ]] || missing+=("$i")
          done
          if (( ${#missing[@]} )); then
            echo "‚ùå Missing shard indices: ${missing[*]}"
            echo "   (found $((${#have[@]})) of $expected)"
            exit 1
          fi
      
      - name: Check disk usage
        run: df -h

      - name: Finalize region file
        env:
          REGION: ${{ needs.guard.outputs.region }}
          BLIZZARD_CLIENT_ID_EU_1:     ${{ secrets.BLIZZARD_CLIENT_ID_EU_1 }}
          BLIZZARD_CLIENT_SECRET_EU_1: ${{ secrets.BLIZZARD_CLIENT_SECRET_EU_1 }}
          BLIZZARD_CLIENT_ID_US_1:     ${{ secrets.BLIZZARD_CLIENT_ID_US_1 }}
          BLIZZARD_CLIENT_SECRET_US_1: ${{ secrets.BLIZZARD_CLIENT_SECRET_US_1 }}
          BLIZZARD_CLIENT_ID_TW_1:     ${{ secrets.BLIZZARD_CLIENT_ID_TW_1 }}
          BLIZZARD_CLIENT_SECRET_TW_1: ${{ secrets.BLIZZARD_CLIENT_SECRET_TW_1 }}
          BLIZZARD_CLIENT_ID_KR_1:     ${{ secrets.BLIZZARD_CLIENT_ID_KR_1 }}
          BLIZZARD_CLIENT_SECRET_KR_1: ${{ secrets.BLIZZARD_CLIENT_SECRET_KR_1 }}
          
          BLIZZARD_CLIENT_ID_EU_2:     ${{ secrets.BLIZZARD_CLIENT_ID_EU_2 }}
          BLIZZARD_CLIENT_SECRET_EU_2: ${{ secrets.BLIZZARD_CLIENT_SECRET_EU_2 }}
          BLIZZARD_CLIENT_ID_US_2:     ${{ secrets.BLIZZARD_CLIENT_ID_US_2 }}
          BLIZZARD_CLIENT_SECRET_US_2: ${{ secrets.BLIZZARD_CLIENT_SECRET_US_2 }}
          BLIZZARD_CLIENT_ID_TW_2:     ${{ secrets.BLIZZARD_CLIENT_ID_TW_2 }}
          BLIZZARD_CLIENT_SECRET_TW_2: ${{ secrets.BLIZZARD_CLIENT_SECRET_TW_2 }}
          BLIZZARD_CLIENT_ID_KR_2:     ${{ secrets.BLIZZARD_CLIENT_ID_KR_2 }}
          BLIZZARD_CLIENT_SECRET_KR_2: ${{ secrets.BLIZZARD_CLIENT_SECRET_KR_2 }}
          
          BLIZZARD_CLIENT_ID_EU_3:     ${{ secrets.BLIZZARD_CLIENT_ID_EU_3 }}
          BLIZZARD_CLIENT_SECRET_EU_3: ${{ secrets.BLIZZARD_CLIENT_SECRET_EU_3 }}
          BLIZZARD_CLIENT_ID_US_3:     ${{ secrets.BLIZZARD_CLIENT_ID_US_3 }}
          BLIZZARD_CLIENT_SECRET_US_3: ${{ secrets.BLIZZARD_CLIENT_SECRET_US_3 }}
          BLIZZARD_CLIENT_ID_TW_3:     ${{ secrets.BLIZZARD_CLIENT_ID_TW_3 }}
          BLIZZARD_CLIENT_SECRET_TW_3: ${{ secrets.BLIZZARD_CLIENT_SECRET_TW_3 }}
          BLIZZARD_CLIENT_ID_KR_3:     ${{ secrets.BLIZZARD_CLIENT_ID_KR_3 }}
          BLIZZARD_CLIENT_SECRET_KR_3: ${{ secrets.BLIZZARD_CLIENT_SECRET_KR_3 }}
          
          BLIZZARD_CLIENT_ID_EU_4:     ${{ secrets.BLIZZARD_CLIENT_ID_EU_4 }}
          BLIZZARD_CLIENT_SECRET_EU_4: ${{ secrets.BLIZZARD_CLIENT_SECRET_EU_4 }}
          BLIZZARD_CLIENT_ID_US_4:     ${{ secrets.BLIZZARD_CLIENT_ID_US_4 }}
          BLIZZARD_CLIENT_SECRET_US_4: ${{ secrets.BLIZZARD_CLIENT_SECRET_US_4 }}
          BLIZZARD_CLIENT_ID_TW_4:     ${{ secrets.BLIZZARD_CLIENT_ID_TW_4 }}
          BLIZZARD_CLIENT_SECRET_TW_4: ${{ secrets.BLIZZARD_CLIENT_SECRET_TW_4 }}
          BLIZZARD_CLIENT_ID_KR_4:     ${{ secrets.BLIZZARD_CLIENT_ID_KR_4 }}
          BLIZZARD_CLIENT_SECRET_KR_4: ${{ secrets.BLIZZARD_CLIENT_SECRET_KR_4 }}
          
          BLIZZARD_CLIENT_ID_EU_5:     ${{ secrets.BLIZZARD_CLIENT_ID_EU_5 }}
          BLIZZARD_CLIENT_SECRET_EU_5: ${{ secrets.BLIZZARD_CLIENT_SECRET_EU_5 }}
          BLIZZARD_CLIENT_ID_US_5:     ${{ secrets.BLIZZARD_CLIENT_ID_US_5 }}
          BLIZZARD_CLIENT_SECRET_US_5: ${{ secrets.BLIZZARD_CLIENT_SECRET_US_5 }}
          BLIZZARD_CLIENT_ID_TW_5:     ${{ secrets.BLIZZARD_CLIENT_ID_TW_5 }}
          BLIZZARD_CLIENT_SECRET_TW_5: ${{ secrets.BLIZZARD_CLIENT_SECRET_TW_5 }}
          BLIZZARD_CLIENT_ID_KR_5:     ${{ secrets.BLIZZARD_CLIENT_ID_KR_5 }}
          BLIZZARD_CLIENT_SECRET_KR_5: ${{ secrets.BLIZZARD_CLIENT_SECRET_KR_5 }}
          
          BLIZZARD_CLIENT_ID_EU_429:     ${{ secrets.BLIZZARD_CLIENT_ID_EU_429 }}
          BLIZZARD_CLIENT_SECRET_EU_429: ${{ secrets.BLIZZARD_CLIENT_SECRET_EU_429 }}
          BLIZZARD_CLIENT_ID_US_429:     ${{ secrets.BLIZZARD_CLIENT_ID_US_429 }}
          BLIZZARD_CLIENT_SECRET_US_429: ${{ secrets.BLIZZARD_CLIENT_SECRET_US_429 }}
          BLIZZARD_CLIENT_ID_TW_429:     ${{ secrets.BLIZZARD_CLIENT_ID_TW_429 }}
          BLIZZARD_CLIENT_SECRET_TW_429: ${{ secrets.BLIZZARD_CLIENT_SECRET_TW_429 }}
          BLIZZARD_CLIENT_ID_KR_429:     ${{ secrets.BLIZZARD_CLIENT_ID_KR_429 }}
          BLIZZARD_CLIENT_SECRET_KR_429: ${{ secrets.BLIZZARD_CLIENT_SECRET_KR_429 }}
          # finalize doesn‚Äôt actually use these, but the script reads them at import time
          BATCH_SIZE: "1"
          BATCH_ID: "0"
          TOTAL_BATCHES: "1"
          OFFSET: "0"
          LIMIT: "1"
        run: |
          set -euo pipefail
          echo "Merging batches and writing final region_${REGION}.lua‚Ä¶"
          python sync_pvp.py --mode finalize --region "$REGION"

      - name: Split oversized region Lua (if >‚ÄÜ49‚ÄØMB)
        id: split
        run: |
          REGION="${{ needs.guard.outputs.region }}"
          MAIN="region_${REGION}.lua"
          if [ -f "$MAIN" ]; then
            SIZE=$(stat -c%s "$MAIN")
            # 49 MiB = 49 * 1024 * 1024 = 51380224 bytes
            if [ "$SIZE" -gt 51380224 ]; then
              echo "üîé Splitting $MAIN ($SIZE bytes) into <49 MB parts..."
              split --bytes=49M --numeric-suffixes=1 --suffix-length=2 \
                    "$MAIN" "region_${REGION}-part-"
              rm "$MAIN"
              i=1
              for chunk in region_${REGION}-part-*; do
                mv "$chunk" "region_${REGION}-${i}.lua"
                echo " ‚úÖ Created region_${REGION}-${i}.lua ($(stat -c%s "region_${REGION}-${i}.lua") bytes)"
                i=$((i+1))
              done
            fi
          fi

      - name: Normalize TOC, purge stale region files, and rewrite list
        shell: bash
        run: |
          set -euo pipefail
          TOC="RatedStats_Achiev.toc"
          [[ -f "$TOC" ]] || { echo "‚ùå $TOC not found"; exit 1; }

          # 1) Normalize CRLF -> LF so regex anchors work
          awk '{ sub("\r$", ""); print }' "$TOC" > "$TOC.tmp" && mv "$TOC.tmp" "$TOC"

          # 2) Decide which files to KEEP per region:
          #    Prefer split parts if any exist; otherwise keep single file.
          declare -a keep_list=()
          declare -A keep_map=()
          for r in eu us tw kr; do
            shopt -s nullglob
            parts=( region_${r}_part*.lua region_${r}-*.lua )
            shopt -u nullglob
            if (( ${#parts[@]} )); then
              # natural sort
              mapfile -t parts_sorted < <(printf '%s\n' "${parts[@]}" | sort -V)
              keep_list+=( "${parts_sorted[@]}" )
              for f in "${parts_sorted[@]}"; do keep_map["$f"]=1; done
            elif [[ -f "region_${r}.lua" ]]; then
              keep_list+=( "region_${r}.lua" )
              keep_map["region_${r}.lua"]=1
            fi
          done

          # 3) Remove any stale region files NOT in keep_list (prevents double loads in-game)
          for r in eu us tw kr; do
            shopt -s nullglob
            all=( region_${r}.lua region_${r}_part*.lua region_${r}-*.lua )
            shopt -u nullglob
            for f in "${all[@]}"; do
              if [[ -e "$f" && -z "${keep_map[$f]+x}" ]]; then
                echo "üßπ Removing stale file: $f"
                git rm -f --quiet "$f" || rm -f "$f"
              fi
            done
          done

          # 4) Rewrite the TOC file: drop any old region_* lines and old achievements.lua,
          #    then append the fresh list + a single achievements.lua once.
          tmp="$(mktemp)"
          # keep all non-region/achievements lines (allow leading/trailing spaces)
          sed -E '/^[[:space:]]*(region_.*\.lua|achievements\.lua)[[:space:]]*$/d' "$TOC" > "$tmp"
          for f in "${keep_list[@]}"; do
            echo "$f" >> "$tmp"
          done
          echo "achievements.lua" >> "$tmp"
          printf '\n' >> "$tmp"
          mv "$tmp" "$TOC"

          echo "üîé Rewritten $TOC entries:"
          printf '  - %s\n' "${keep_list[@]:-<none>}"
          # fail hard if nothing listed
          grep -Eq '^[[:space:]]*region_.*\.lua[[:space:]]*$' "$TOC" || { echo "‚ùå No region files listed"; exit 1; }
      
      - name: Commit region Lua chunks (bypassing LFS)
        run: |
          REGION="${{ needs.guard.outputs.region }}"
          TZ="Europe/London" ; export TZ
          DATE="$(date +%F)"  # YYYY-MM-DD
          git config user.name "GitHub Action"
          git config user.email "action@github.com"
          # Override LFS filters so actual Lua content is committed, not pointer
          git -c filter.lfs.clean=cat \
              -c filter.lfs.smudge=cat \
              -c filter.lfs.process= \
              -c filter.lfs.required=false \
            add region_${REGION}*.lua
          git add RatedStats_Achiev.toc
          git commit -m "Finalize PvP Sync for ${REGION} - ${DATE}" || true

      - name: Check if all 4 regions are finalized (main branch only)
        if: ${{ github.ref_name == 'main' }}
        run: |
          set -euo pipefail
          BRANCH="${{ github.event.workflow_run.head_branch || inputs.branch }}"
          REGION="${{ needs.guard.outputs.region }}"
          sha="${{ github.event.workflow_run.head_sha || inputs.sha }}"

          TRACKDIR=".github/workflows/finalize-tracker/main/$sha"
          mkdir -p "$TRACKDIR"
          echo "$REGION" > "$TRACKDIR/$REGION.done"

          count=$(ls "$TRACKDIR"/*.done 2>/dev/null | wc -l)
          echo "Found $count of 4 regions done so far for sha=$sha"

          if (( count < 4 )); then
            echo "üü° $REGION done, but waiting for all 4 regions on main branch ‚Üí skipping commit/tag"
            exit 0
          fi

          echo "‚úÖ All 4 regions finalized for sha=$sha ‚Äî continuing to commit + tag"

          # Cleanup everything except README.txt
          find "$TRACKDIR" -type f ! -name 'README.txt' -delete

      - name: Push finalized commit
        run: |
          set -euo pipefail
          BRANCH="${{ github.event.workflow_run.head_branch || inputs.branch }}"
          tries=0; max=6
          while :; do
            if git push origin "HEAD:${BRANCH}"; then
              echo "‚úÖ Pushed ${BRANCH}"
              break
            fi
            tries=$((tries+1))
            if (( tries >= max )); then
              echo "‚ùå Push failed after ${max} attempts"
              exit 1
            fi
            echo "‚ö†Ô∏è Push rejected; remote advanced‚Äîattempting to rebase on attempt ${tries}/${max}"
            git fetch origin "${BRANCH}"

            # üîß Safely rebase our local commit(s) (usually just region_*.lua)
            # onto the updated remote, stashing any unstaged changes.
            # Rebase while bypassing LFS filters
            git -c filter.lfs.clean=cat \
                -c filter.lfs.smudge=cat \
                -c filter.lfs.process= \
                -c filter.lfs.required=false \
              rebase --autostash origin/"${BRANCH}"

            # Git rebase --autostash handles unstaged files automatically.
            # If rebase fails due to conflict, stop early‚Äîmanual intervention may be needed.
            sleep $((2 * tries))
          done

      - name: Tag this commit (vA.BB [+ -beta] + date)
        run: |
          set -euo pipefail
          BRANCH="${{ github.event.workflow_run.head_branch || inputs.branch }}"

          # Use London time for date suffix
          TZ="Europe/London" ; export TZ
          DATE="$(date +%F)"  # YYYY-MM-DD

          # Make sure we see *all* tags
          git fetch --tags --force --prune

          if [[ "$BRANCH" == "dev" ]]; then
            # dev: only look at tags ending in -beta
            TAGS=$(git tag -l 'v*-beta')
          else
            # main: only look at tags NOT ending in -beta
            TAGS=$(git tag -l 'v*' | grep -Ev -- '-beta$' || true)
          fi

          # Extract vA.BB from matching tags
          BASES=$(sed -E 's/^(v[0-9]+\.[0-9]+).*/\1/' <<< "$TAGS" | sort -u -V)
          last_base=$(tail -n1 <<< "$BASES")

          if [[ -z "${last_base:-}" ]]; then
            major=1
            minor=0
          else
            major="$(sed -E 's/^v([0-9]+)\..*/\1/' <<< "$last_base")"
            minor="$(sed -E 's/^v[0-9]+\.([0-9]+).*/\1/' <<< "$last_base")"
          fi

          next_minor=$((10#$minor + 1))
          width="${#minor}"
          minor_fmt=$(printf "%0${width}d" "${next_minor}")
          base="v${major}.${minor_fmt}"

          suffix=""
          if [[ "$BRANCH" == "dev" ]]; then
            suffix="-beta"
          fi
          tag="${base}-${DATE}${suffix}"

          # Idempotency check: skip if this tag already exists
          if git rev-parse -q --verify "refs/tags/${tag}" >/dev/null; then
            echo "‚ÑπÔ∏è Tag ${tag} already exists; nothing to do."
            exit 0
          fi

          git tag -a "${tag}" -m "Automated tag for ${BRANCH} ${tag}"
          git push origin "refs/tags/${tag}"

      - name: Clean up temp DB outputs
        run: |
          rm -rf _dl partial_outputs || true
